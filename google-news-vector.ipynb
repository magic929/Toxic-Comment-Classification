{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## summary\n",
    "\n",
    "this kernel using the google-news-vector-300 to trian the bilstm network.\n",
    "\n",
    "we try two dataset raw, pre-process(refer to the pre-fit-embedding.ipynb)\n",
    "\n",
    "using raw dataset the Private Score is 0.93930 and Public Score is 0.93277\n",
    "\n",
    "using pre-process dataset the Pricate Score is 0.96446 and the Public Score is 0.96443\n",
    "\n",
    "the train method is bilstm network using keras(backend tensorflow) in kaggle kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin.gz\n",
      "/kaggle/input/processed/process_train.csv\n",
      "/kaggle/input/processed/process_test.csv\n",
      "/kaggle/input/jigsaw-toxic-comment-classification-challenge/test_labels.csv\n",
      "/kaggle/input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv\n",
      "/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv\n",
      "/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "embedding_index = KeyedVectors.load_word2vec_format('../input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_train = pd.read_csv('../input/processed/process_train.csv')\n",
    "pre_train_comment = pre_train['comment_text'].fillna('')\n",
    "pre_test = pd.read_csv('../input/processed/process_test.csv')\n",
    "pre_test_comment = pre_test['comment_text'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pre_train[labels].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D,Bidirectional\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(pre_train_comment))\n",
    "pre_train_comment = tokenizer.texts_to_sequences(pre_train_comment)\n",
    "pre_test_comment = tokenizer.texts_to_sequences(pre_test_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 200\n",
    "X_t = pad_sequences(pre_train_comment, maxlen=maxlen)\n",
    "X_te = pad_sequences(pre_test_comment, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadEmbeddingMatrix(pre_vector):\n",
    "    embed_size = 300\n",
    "    embeddings_index = dict()\n",
    "    for word in pre_vector.wv.vocab:\n",
    "        embeddings_index[word] = pre_vector.word_vec(word)\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "    \n",
    "    gc.collect()\n",
    "    all_embs = np.stack(list(embeddings_index.values()))\n",
    "    emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "    \n",
    "    nb_words = len(tokenizer.word_index)\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    gc.collect()\n",
    "\n",
    "    embeddedCount = 0\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        i -= 1\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            embeddedCount += 1\n",
    "    print('total embedded:', embeddedCount,'common words')\n",
    "    \n",
    "    del(embeddings_index)\n",
    "    gc.collect()\n",
    "\n",
    "    return embedding_matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(211620, 300)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(maxlen, ))\n",
    "x = Embedding(len(tokenizer.word_index), embedding_matrix.shape[1],weights=[embedding_matrix],trainable=False)(inp)\n",
    "x = Bidirectional(LSTM(60, return_sequences=True,name='lstm_layer',dropout=0.1,recurrent_dropout=0.1))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(50, activation='relu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(6, activation='sigmoid')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint('../input/weights_word2vec.best.hdf5', monitor='val_loss', verbose=1, save_best_only=True, mode='max')\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"max\", patience=20)\n",
    "callbacks_list = [checkpoint, early]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 200, 300)          63486000  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200, 120)          173280    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                6050      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 63,665,636\n",
      "Trainable params: 179,636\n",
      "Non-trainable params: 63,486,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/5\n",
      "143613/143613 [==============================] - 725s 5ms/step - loss: 0.1054 - accuracy: 0.9694 - val_loss: 0.0693 - val_accuracy: 0.9775\n",
      "\n",
      "Epoch 00001: val_loss improved from -inf to 0.06927, saving model to ../input/weights_word2vec.best.hdf5\n",
      "Epoch 2/5\n",
      "143613/143613 [==============================] - 706s 5ms/step - loss: 0.0661 - accuracy: 0.9784 - val_loss: 0.0599 - val_accuracy: 0.9798\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.06927\n",
      "Epoch 3/5\n",
      "143613/143613 [==============================] - 702s 5ms/step - loss: 0.0583 - accuracy: 0.9802 - val_loss: 0.0570 - val_accuracy: 0.9804\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.06927\n",
      "Epoch 4/5\n",
      "143613/143613 [==============================] - 704s 5ms/step - loss: 0.0544 - accuracy: 0.9812 - val_loss: 0.0548 - val_accuracy: 0.9810\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.06927\n",
      "Epoch 5/5\n",
      "143613/143613 [==============================] - 697s 5ms/step - loss: 0.0516 - accuracy: 0.9818 - val_loss: 0.0532 - val_accuracy: 0.9815\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.06927\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 5\n",
    "result = model.fit(X_t,y, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = model.predict(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv\")\n",
    "sample_submission[labels] = y_test\n",
    "\n",
    "sample_submission.to_csv(\"submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
