{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read the dataset\n",
    "\n",
    "1. first, see the data sample. \n",
    "```\n",
    "the task is text multi-classification in six(toxic, server_toxic, obscene, threat, insult, indentity_hate)\n",
    "from the sample data, we konw one text with some labels, and same text data is no labels(all 0)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('dataset/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9144</th>\n",
       "      <td>185da65a1b870e3c</td>\n",
       "      <td>The last two episode of Jon &amp; Kate Plus 8 have...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157162</th>\n",
       "      <td>d9bfa04a9c3be8dd</td>\n",
       "      <td>Clarified. I think i was able to work it, desp...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91755</th>\n",
       "      <td>f5478b2aab845bb4</td>\n",
       "      <td>And a source claiming that Evanescence is a go...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15128</th>\n",
       "      <td>27f4c1a114603d00</td>\n",
       "      <td>\"\\n\\nWilla gets the rights to the album \"\"Sexy...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76710</th>\n",
       "      <td>cd6fa5923f7c21b7</td>\n",
       "      <td>\"#REDIRECT Talk:\"\"Polish Operation\"\" of the NK...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119620</th>\n",
       "      <td>7f8ef79020a98b40</td>\n",
       "      <td>\"\\nThe F-14 was designed as a dogfighter first...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2129</th>\n",
       "      <td>05c1f66548965da9</td>\n",
       "      <td>\"\\n\\n Unapproved bot. \\n\\nYou appear to be usi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42523</th>\n",
       "      <td>717af680e51654a6</td>\n",
       "      <td>Disclosing my affliation to M. Sanjayan, whose...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52804</th>\n",
       "      <td>8d29a15eecb0190f</td>\n",
       "      <td>The fact that your only contribution to the en...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87874</th>\n",
       "      <td>eb13473631370d56</td>\n",
       "      <td>I like the suggestion, and am impressed with t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "9144    185da65a1b870e3c  The last two episode of Jon & Kate Plus 8 have...   \n",
       "157162  d9bfa04a9c3be8dd  Clarified. I think i was able to work it, desp...   \n",
       "91755   f5478b2aab845bb4  And a source claiming that Evanescence is a go...   \n",
       "15128   27f4c1a114603d00  \"\\n\\nWilla gets the rights to the album \"\"Sexy...   \n",
       "76710   cd6fa5923f7c21b7  \"#REDIRECT Talk:\"\"Polish Operation\"\" of the NK...   \n",
       "119620  7f8ef79020a98b40  \"\\nThe F-14 was designed as a dogfighter first...   \n",
       "2129    05c1f66548965da9  \"\\n\\n Unapproved bot. \\n\\nYou appear to be usi...   \n",
       "42523   717af680e51654a6  Disclosing my affliation to M. Sanjayan, whose...   \n",
       "52804   8d29a15eecb0190f  The fact that your only contribution to the en...   \n",
       "87874   eb13473631370d56  I like the suggestion, and am impressed with t...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "9144        0             0        0       0       0              0  \n",
       "157162      0             0        0       0       0              0  \n",
       "91755       0             0        0       0       0              0  \n",
       "15128       0             0        0       0       0              0  \n",
       "76710       0             0        0       0       0              0  \n",
       "119620      0             0        0       0       0              0  \n",
       "2129        0             0        0       0       0              0  \n",
       "42523       0             0        0       0       0              0  \n",
       "52804       0             0        0       0       0              0  \n",
       "87874       0             0        0       0       0              0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>none</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.095844</td>\n",
       "      <td>0.009996</td>\n",
       "      <td>0.052948</td>\n",
       "      <td>0.002996</td>\n",
       "      <td>0.049364</td>\n",
       "      <td>0.008805</td>\n",
       "      <td>0.898321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.294379</td>\n",
       "      <td>0.099477</td>\n",
       "      <td>0.223931</td>\n",
       "      <td>0.054650</td>\n",
       "      <td>0.216627</td>\n",
       "      <td>0.093420</td>\n",
       "      <td>0.302226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               toxic   severe_toxic        obscene         threat  \\\n",
       "count  159571.000000  159571.000000  159571.000000  159571.000000   \n",
       "mean        0.095844       0.009996       0.052948       0.002996   \n",
       "std         0.294379       0.099477       0.223931       0.054650   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "              insult  identity_hate           none  \n",
       "count  159571.000000  159571.000000  159571.000000  \n",
       "mean        0.049364       0.008805       0.898321  \n",
       "std         0.216627       0.093420       0.302226  \n",
       "min         0.000000       0.000000       0.000000  \n",
       "25%         0.000000       0.000000       1.000000  \n",
       "50%         0.000000       0.000000       1.000000  \n",
       "75%         0.000000       0.000000       1.000000  \n",
       "max         1.000000       1.000000       1.000000  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "train['none'] = 1 - train[labels].max(axis=1)\n",
    "train['comment_text'] = train['comment_text'].str.lower()\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract the word feature\n",
    "\n",
    "in the public notebook, the higher score kernel that feature used the tf/idf, then we use the pre-trained word embedding to instead it.\n",
    "\n",
    "we use the two pre-trained word embedding (google news vetocr/ fasttext)\n",
    "\n",
    "before using it, we should fix the text vocab to fit the pre-trained word embedding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the method for fitting the pre-train word emebedding is\n",
    "\n",
    "we check the percentage that how many words in the pre-trained embedding and text. then we get the oov(out of vocabulary) and clean / modifty the oov to get the higer percentage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the vocabulary method\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "tqdm.pandas()\n",
    "def build_vocab(sentences, verbose=True):\n",
    "    vocab = {}\n",
    "    for sentence in tqdm(sentences, disable=(not verbose)):\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159571/159571 [00:01<00:00, 91687.45it/s] \n",
      "100%|██████████| 159571/159571 [00:02<00:00, 69232.52it/s]\n"
     ]
    }
   ],
   "source": [
    "# get the vocab\n",
    "sentences = train['comment_text'].progress_apply(lambda x: x.split()).values\n",
    "vocab = build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the google_news_vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the pre-trained embedding\n",
    "from gensim.models import KeyedVectors\n",
    "embedding_index = KeyedVectors.load_word2vec_format('./model/GoogleNews-vectors-negative300.bin/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the pre-trained embedding\n",
    "from gensim.models import KeyedVectors\n",
    "embedding_index =  KeyedVectors.load_word2vec_format('./model/crawl-300d-2M.vec/crawl-300d-2M.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the coverage \n",
    "# embedding of (vocab, all text)\n",
    "import operator\n",
    "def check_coverage(vocab, embeddings_index):\n",
    "    a = {}\n",
    "    oov = {}\n",
    "    k = 0\n",
    "    i = 0\n",
    "    for word in tqdm(vocab):\n",
    "        try:\n",
    "            a[word] = embeddings_index[word]\n",
    "            k += vocab[word]\n",
    "        except:\n",
    "            oov[word] = vocab[word]\n",
    "            i += vocab[word]\n",
    "            pass\n",
    "    \n",
    "    print('Found embedding for {:.2%} of vocab'.format(len(a) / len(vocab)))\n",
    "    print('Found embedding for {:.2%} of all text'.format(k / (k + i)))\n",
    "    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "    \n",
    "    return sorted_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 470340/470340 [00:01<00:00, 427171.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embedding for 23.44% of vocab\n",
      "Found embedding for 88.99% of all text\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab, embedding_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"i'm\", 17305),\n",
       " (\"i've\", 8507),\n",
       " ('(utc)', 5780),\n",
       " ('article,', 5596),\n",
       " ('page,', 5415),\n",
       " (\"i'll\", 5175),\n",
       " ('(talk)', 4250),\n",
       " (\"i'd\", 3392),\n",
       " ('also,', 3372),\n",
       " ('so,', 3214)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# out of vocabulary\n",
    "oov[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the symbol\n",
    "\n",
    "# def clean_text(x):\n",
    "#     x = str(x)\n",
    "#     for punct in \"/-—–\":\n",
    "#         x = x.replace(punct, ' ')\n",
    "#     for punct in '&':\n",
    "#         x = x.replace(punct, f' {punct} ')\n",
    "#     for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n",
    "#         x = x.replace(punct, '')\n",
    "#     return x\n",
    "\n",
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    for punct in '—?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n",
    "        x = x.replace(punct, '')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159571/159571 [00:02<00:00, 64986.43it/s]\n",
      "100%|██████████| 159571/159571 [00:02<00:00, 75222.13it/s]\n"
     ]
    }
   ],
   "source": [
    "train['comment_text'] = train['comment_text'].progress_apply(lambda x: clean_text(x))\n",
    "sentences = train['comment_text'].apply(lambda x: x.split())\n",
    "vocab = build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 253453/253453 [00:00<00:00, 412550.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embedding for 41.45% of vocab\n",
      "Found embedding for 97.35% of all text\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab, embedding_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('npov', 1480),\n",
       " ('wikipediahi', 713),\n",
       " ('3rr', 657),\n",
       " ('fucksex', 624),\n",
       " ('yourselfgo', 621),\n",
       " ('verticalaligntop', 606),\n",
       " ('width100', 590),\n",
       " ('wikipediaquestions', 563),\n",
       " ('stylewidth', 553),\n",
       " ('mothjer', 489)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the number \n",
    "\n",
    "import re\n",
    "\n",
    "def clean_numbers(x):\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', \"###\", x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159571/159571 [00:05<00:00, 31061.11it/s]\n",
      "100%|██████████| 159571/159571 [00:01<00:00, 141538.14it/s]\n",
      "100%|██████████| 159571/159571 [00:01<00:00, 86858.84it/s]\n"
     ]
    }
   ],
   "source": [
    "train['comment_text'] = train['comment_text'].progress_apply(lambda x: clean_numbers(x))\n",
    "sentences = train['comment_text'].progress_apply(lambda x: x.split())\n",
    "vocab = build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217359/217359 [00:00<00:00, 450253.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embedding for 30.55% of vocab\n",
      "Found embedding for 87.24% of all text\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab, embedding_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('to', 297269),\n",
       " ('of', 224448),\n",
       " ('and', 223455),\n",
       " ('a', 214860),\n",
       " ('doesnt', 6698),\n",
       " ('didnt', 5772),\n",
       " ('isnt', 4604),\n",
       " ('wikipedias', 2818),\n",
       " ('contribs', 2347),\n",
       " ('wasnt', 2256),\n",
       " ('enwikipediaorg', 2009),\n",
       " ('faggot', 1981),\n",
       " ('shouldnt', 1548),\n",
       " ('npov', 1539),\n",
       " ('afd', 1353),\n",
       " ('wikiproject', 1332),\n",
       " ('wikipedian', 1267),\n",
       " ('barnstar', 1024),\n",
       " ('infobox', 946),\n",
       " ('wikipedians', 900),\n",
       " ('aligntop', 827),\n",
       " ('behaviour', 785),\n",
       " ('helpme', 758),\n",
       " ('rfa', 748),\n",
       " ('wikipediahi', 713),\n",
       " ('faggots', 682),\n",
       " ('colorf5fffa', 679),\n",
       " ('3rr', 671),\n",
       " ('fucksex', 624),\n",
       " ('yourselfgo', 621)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix the special or mispell words\n",
    "\n",
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "mispell_dict = {\n",
    "    'fucksex': 'fuck sex',\n",
    "    'yourselfgo': 'yourself go',\n",
    "    'mothjer': 'mother',\n",
    "    'philippineslong': 'philippines long',\n",
    "    'offfuck': 'off fuck',\n",
    "    'bitchesfuck': 'bitches fuck',\n",
    "    'youfuck': 'you fuck',\n",
    "    'bitchfuck': 'bitch fuck',\n",
    "    'talkcontribs': 'talk contribs',\n",
    "    'criminalwar': 'criminal war',\n",
    "    'penissmall': 'penis small',\n",
    "    'securityfuck': 'security fuck'\n",
    "}\n",
    "    \n",
    "# mispell_dict = {\n",
    "#     'doesnt': 'does not',\n",
    "#     'isnt': 'is not',\n",
    "#     'wasnt': 'was not',\n",
    "#     'contribs': 'contributions',\n",
    "#     'npov': 'neutral point of view',\n",
    "#     'shouldnt': 'should not',\n",
    "#     'wikiproject': 'wiki project',\n",
    "#     'afd': 'alternative for germany',\n",
    "#     'aligntop': 'align top',\n",
    "#     'infobox': 'info box',\n",
    "#     'behaviour': 'behavior',\n",
    "#     'helpme': 'help me',\n",
    "#     'wikipediahi': 'wikipedia hi',\n",
    "#     'fucksex': 'fuck sex',\n",
    "#     'yourselfgo': 'yourself go',\n",
    "#     'didnt': 'did not',\n",
    "#     'mothjer': 'mother',\n",
    "#     'talkpage': 'talk page',\n",
    "#     'wikipediaquestions': 'wikipedia questions',\n",
    "#     'hasnt': 'has not',\n",
    "#     'userpage': 'user page',\n",
    "#     'arbcom': 'arbitration committee',\n",
    "#     'rfa': 'radio free asia',\n",
    "#     'wikipedian': 'wikipedia',\n",
    "#     'wikipedias': 'wikipedia',\n",
    "#     'faggot': 'fag got',\n",
    "#     'bitchesfuck': 'bitches fuck',\n",
    "#     'offfuck': 'off fuck',\n",
    "#     'organisation': 'organization',\n",
    "#     'sexsex': 'sex sex',\n",
    "#     'youfuck': 'you fuck',\n",
    "#     'bitchfuck': 'bitch fuck'\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "mispellings, misepplings_re = _get_mispell(mispell_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_typical_misspell(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "    \n",
    "    return misepplings_re.sub(replace, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the html(css etc.) text\n",
    "\n",
    "def clean_html(x):\n",
    "    x = re.sub('color[0-9A-Fa-f]+', '', x)\n",
    "    x = re.sub('width[0-9]+', '', x)\n",
    "    x = re.sub('border[0-9]+px', '', x)\n",
    "    x = re.sub('oldid#+', '', x)\n",
    "    x = re.sub('verticalaligntop', '', x)\n",
    "    x = re.sub('[a-z1-9]*wikipedia[a-z1-9]*', '', x)\n",
    "    x = re.sub('cellpadding[0-9]+', '', x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159571/159571 [00:01<00:00, 80684.55it/s]\n",
      "100%|██████████| 159571/159571 [00:04<00:00, 32381.65it/s]\n",
      "100%|██████████| 159571/159571 [00:01<00:00, 139735.70it/s]\n",
      "100%|██████████| 159571/159571 [00:02<00:00, 66610.09it/s]\n",
      "100%|██████████| 159571/159571 [00:02<00:00, 71941.66it/s]\n"
     ]
    }
   ],
   "source": [
    "train['comment_text'] = train['comment_text'].progress_apply(lambda x: replace_typical_misspell(x))\n",
    "train['comment_text'] = train['comment_text'].progress_apply(lambda x: clean_html(x))\n",
    "sentences = train['comment_text'].progress_apply(lambda x: x.split())\n",
    "to_remove = ['stylewidth', 'stylebackground', 'sockpuppetry', 'deneidaccess', 'pagedelete', 'stylewidth', 'verticalaligntop', 'editwarring']\n",
    "sentences = [[word for word in sentences if not word in to_remove] for sentences in tqdm(sentences)]\n",
    "vocab = build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250608/250608 [00:00<00:00, 398853.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embedding for 41.91% of vocab\n",
      "Found embedding for 97.52% of all text\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab, embedding_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('npov', 1480),\n",
       " ('3rr', 657),\n",
       " ('fggt', 480),\n",
       " ('gfdl', 444),\n",
       " ('wprs', 393),\n",
       " ('proassadhanibal911youre', 345),\n",
       " ('ricehey', 329),\n",
       " ('threerevert', 321),\n",
       " ('notrhbysouthbanof', 308),\n",
       " ('classmainpagebg', 304),\n",
       " ('wpnpov', 299),\n",
       " ('bunksteve', 278),\n",
       " ('marcolfuck', 260),\n",
       " ('boymamas', 258),\n",
       " ('wpblp', 253),\n",
       " ('ytmndin', 238),\n",
       " ('tommy2010', 228),\n",
       " ('wpor', 227),\n",
       " ('youbollocks', 217),\n",
       " ('basteredbastered', 217)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = ['to', 'of', 'and', 'a', 'stylewidth', 'stylebackground', 'sockpuppetry', 'deneidaccess', 'pagedelete', 'stylewidth']\n",
    "def delete_stop_words(x, stop_words):\n",
    "    x = ' '.join([word for word in x.split() if not word in stop_words])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all process\n",
    "\n",
    "def process(text):\n",
    "    text['comment_text'] = text['comment_text'].str.lower()\n",
    "    text['comment_text'] = text['comment_text'].progress_apply(lambda x: clean_text(x))\n",
    "    # text['comment_text'] = text['comment_text'].progress_apply(lambda x: clean_numbers(x))\n",
    "    text['comment_text'] = text['comment_text'].progress_apply(lambda x: replace_typical_misspell(x))\n",
    "    text['comment_text'] = text['comment_text'].progress_apply(lambda x: clean_html(x))\n",
    "    text['comment_text'] = text['comment_text'].progress_apply(lambda x: delete_stop_words(x, to_remove))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153164/153164 [00:02<00:00, 66684.02it/s]\n",
      "100%|██████████| 153164/153164 [00:01<00:00, 90443.63it/s]\n",
      "100%|██████████| 153164/153164 [00:04<00:00, 36391.68it/s]\n",
      "100%|██████████| 153164/153164 [00:02<00:00, 65601.56it/s]\n"
     ]
    }
   ],
   "source": [
    "# save the pre-test\n",
    "test = pd.read_csv('./dataset/test.csv')\n",
    "test = process(test)\n",
    "test.to_csv('./fasttext_dataset/process_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159571/159571 [00:02<00:00, 64024.58it/s]\n",
      "100%|██████████| 159571/159571 [00:01<00:00, 82303.26it/s]\n",
      "100%|██████████| 159571/159571 [00:04<00:00, 32199.12it/s]\n",
      "100%|██████████| 159571/159571 [00:02<00:00, 59678.26it/s]\n"
     ]
    }
   ],
   "source": [
    "# save the pre-train\n",
    "train = pd.read_csv('./dataset/train.csv')\n",
    "train = process(train)\n",
    "train.to_csv('./fasttext_dataset/process_train.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
