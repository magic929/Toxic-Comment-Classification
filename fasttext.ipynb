{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## summary\n",
    "\n",
    "this kernel using the fasttext-300 to trian the bilstm network.\n",
    "\n",
    "we try two dataset raw, pre-process(refer to the pre-fit-embedding.ipynb)\n",
    "\n",
    "using raw dataset the Private Score is 0.95180 and Public Score is 0.95430\n",
    "\n",
    "using pre-process dataset the Pricate Score is 0.96446 and the Public Score is 0.96443\n",
    "\n",
    "the train method is bilstm network using keras(backend tensorflow) in kaggle kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin.gz\n",
      "/kaggle/input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\n",
      "/kaggle/input/processed/process_train.csv\n",
      "/kaggle/input/processed/process_test.csv\n",
      "/kaggle/input/weigths/embedding_matrix.pkl\n",
      "/kaggle/input/jigsaw-toxic-comment-classification-challenge/test_labels.csv\n",
      "/kaggle/input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv\n",
      "/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv\n",
      "/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv\n",
      "/kaggle/input/fasttext-pre/process_train.csv\n",
      "/kaggle/input/fasttext-pre/process_test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_train = pd.read_csv('../input/fasttext-pre/process_train.csv')\n",
    "pre_train_comment = pre_train['comment_text'].fillna('')\n",
    "pre_test = pd.read_csv('../input/fasttext-pre/process_test.csv')\n",
    "pre_test_comment = pre_test['comment_text'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre_train = pd.read_csv('../input/processed/process_train.csv')\n",
    "# pre_train_comment = pre_train['comment_text'].fillna('')\n",
    "# pre_test = pd.read_csv('../input/processed/process_test.csv')\n",
    "# pre_test_comment = pre_test['comment_text'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pre_train[labels].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation,SpatialDropout1D, concatenate\n",
    "from keras.layers import Bidirectional,Bidirectional, GRU, GlobalAveragePooling1D,GlobalMaxPooling1D,GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 30000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(pre_train_comment))\n",
    "pre_train_comment = tokenizer.texts_to_sequences(pre_train_comment)\n",
    "pre_test_comment = tokenizer.texts_to_sequences(pre_test_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 100\n",
    "X_t = pad_sequences(pre_train_comment, maxlen=maxlen)\n",
    "X_te = pad_sequences(pre_test_comment, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import KeyedVectors\n",
    "# embedding_index = KeyedVectors.load_word2vec_format('../input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "from gensim.models import KeyedVectors\n",
    "embedding_index =  KeyedVectors.load_word2vec_format('../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadEmbeddingMatrix(pre_vector):\n",
    "    embed_size = 300\n",
    "    embeddings_index = dict()\n",
    "    for word in pre_vector.wv.vocab:\n",
    "        embeddings_index[word] = pre_vector.word_vec(word)\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "    \n",
    "    gc.collect()\n",
    "    all_embs = np.stack(list(embeddings_index.values()))\n",
    "    emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "    \n",
    "    nb_words = len(tokenizer.word_index)\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    gc.collect()\n",
    "\n",
    "    embeddedCount = 0\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        i -= 1\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            embeddedCount += 1\n",
    "    print('total embedded:', embeddedCount,'common words')\n",
    "    \n",
    "    del(embeddings_index)\n",
    "    gc.collect()\n",
    "\n",
    "    return embedding_matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2000000 word vectors.\n",
      "total embedded: 105087 common words\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "embedding_matrix = loadEmbeddingMatrix(embedding_index)\n",
    "pickle.dump(embedding_matrix, open(\"embedding_matrix.pkl\", \"wb\"))\n",
    "# import pickle\n",
    "# embedding_matrix = pickle.load(open('../input/weigths/embedding_matrix.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(252009, 300)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(maxlen, ))\n",
    "x = Embedding(len(tokenizer.word_index), embedding_matrix.shape[1],weights=[embedding_matrix],trainable=False)(inp)\n",
    "# x = SpatialDropout1D(0.2)(x)\n",
    "x = Bidirectional(LSTM(60, return_sequences=True,name='lstm_layer',dropout=0.1,recurrent_dropout=0.1))(x)\n",
    "x = Bidirectional(GRU(80, return_sequences=True))(x)\n",
    "# avg_pool = GlobalAveragePooling1D()(x)\n",
    "# max_pool = GlobalMaxPooling1D()(x)\n",
    "# conc = concatenate([avg_pool, max_pool])\n",
    "# outp = Dense(6, activation=\"sigmoid\")(conc)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(50, activation='relu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(6, activation='sigmoid')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint('weights_word2vec.best.hdf5', monitor='val_loss', verbose=1, save_best_only=True, mode='max')\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"max\", patience=20)\n",
    "callbacks_list = [checkpoint, early]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 100, 300)          75602700  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 100, 120)          173280    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 100, 160)          96480     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                8050      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 75,880,816\n",
      "Trainable params: 278,116\n",
      "Non-trainable params: 75,602,700\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143613/143613 [==============================] - 1276s 9ms/step - loss: 0.0794 - accuracy: 0.9760 - val_loss: 0.0613 - val_accuracy: 0.9796\n",
      "\n",
      "Epoch 00001: val_loss improved from -inf to 0.06135, saving model to weights_word2vec.best.hdf5\n",
      "Epoch 2/2\n",
      "143613/143613 [==============================] - 1262s 9ms/step - loss: 0.0550 - accuracy: 0.9812 - val_loss: 0.0531 - val_accuracy: 0.9816\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.06135\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 2\n",
    "# with tf.device('/gpu:0'):\n",
    "hist = model.fit(X_t,y, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('weights_word2vec.best.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = model.predict(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv\")\n",
    "sample_submission[labels] = y_test\n",
    "\n",
    "sample_submission.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
